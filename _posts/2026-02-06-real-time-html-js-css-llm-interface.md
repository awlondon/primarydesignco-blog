---
layout: post
title: "Real-Time HTML/JS/CSS as a First-Class LLM Interface"
subtitle: "Turning text-only chat into a live, multimodal surface"
author: Alexander Warren London
contributors:
  - Alexander Warren London
published_at: 2026-02-06T08:00:00-08:00
tags: [ai, interfaces, llms, multimodal, tooling, design-practice]
reading_time: 18
abstract: |
  Contemporary large language models are already capable of producing interactive
  user interfaces, executable logic, and stylistic systems. Yet most chat platforms
  flatten these capabilities into static text. This essay argues that multimodal
  interaction does not require new models or native platform features. By structuring
  prompts to return executable HTML, CSS, and JavaScript alongside prose—and rendering
  that code in real time—users can convert existing text-based chatbots into live,
  inspectable, and extensible interaction surfaces.
---

<!-- Embedded live demo (top-of-article) -->
<iframe
  src="https://awlondon.github.io/maya-dev-ui/"
  style="width:100%; height:520px; border:1px solid #ddd; border-radius:8px;"
  loading="lazy">
</iframe>

## The Interface Bottleneck

{% include ad.html %}

## The Prompt-Wrapper Pattern

## What Existing Platforms Already Hint At

{% include ad.html %}

## The Missing Layer: Real-Time Rendering

## A Reference Implementation: maya-dev-ui

{% include ad.html %}

## Why This Works Now (Not Later)

## Limits, Risks, and Failure Modes

{% include ad.html %}

## Conclusion: Text Is the Control Plane, Not the Interface

## Contextual Recommendation

[Contextual link to relevant PrimaryDesignCo.com page goes here.]

## References
